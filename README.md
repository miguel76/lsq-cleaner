# lsq-cleaner
A tool to clean and extract SPARQL logs information from dumps generated by the [Linked SPARQL Queries (LSQ) project](http://lsq.aksw.org/)

The LSQ project collected logs from several SPARQL endpoints and mapped them to a [common RDF vocabulary](http://lsq.aksw.org/vocab) visually described [here](http://lsq.aksw.org/v2/concepts/data-model.html).
But due to [this bug](https://github.com/AKSW/LSQ/issues/40), the resulting RDF graph does not reflect the original data and some information is lost (namely specific information about each separate execution of a query).
Furthermore, LSQ files contain, along the basic information about query executions from the logs, derived information on query structure (representation in SPIN-SPARQL notation and triple patterns contained) and local re-execution of queries or part of them.
That information is potentially useful but takes a lot of space and can extracted/computed back from the basic log information.
This tool extracts the basic log information from LSQ files (N-Triples compressed with bz2), using an euristic (based on the order of the triples in the files) to recover as much as possible the information that would have been lost considering the files as RDF. The data is then exported both as RDF (N-Triples compressed with gzip) and as CSV files.

## Extracted Information

For each unique query the information items extracted are the following ones:
- ID of the query;
- full textual representation (SPARQL).

For each execution the information items are the following ones:
- ID of the query executed;
- timestamp of the log entry (should be the time when the query is received);
- hash of the host that sent the query to the endpoint (if available).

In RDF output files this information is represented according to the [LSQ vocabulary](http://lsq.aksw.org/vocab),
while for CSV output there are two kinds of files exported, one for unique queries and one for executions,
with columns corresponding with the information items described above.

## Usage from Command Line (suggested for a single local source)
The only requirement is having [Docker](https://www.docker.com/) installed and running.

Pull image from Docker Hub

```shell
docker pull miguel76/lsq-clean
```

Example usage:

```shell
docker run miguel76/lsq-clean \
    --source data/input/bench-kegg-lsq2.nt.bz2 \
    --queries-csv data/output/bio2rdf-kegg_queries.csv \
    --execs-csv data/output/bio2rdf-kegg_execs.csv \
    --queries-rdf data/output/bio2rdf-kegg_queries.nt.gz \
    --execs-rdf data/output/bio2rdf-kegg_execs.nt.gz \
    --execs-ns http://lsq.aksw.org/sources/kegg/execs/
```

Get usage documentation

```shell
docker run miguel76/lsq-clean --help
```

## Usage in a CWL pipeline (suggested for multiple sources)

In order to support more complex scenarios, some pipelines using the [Common Workflow Language (CWL)](https://www.commonwl.org/) are supplied.
You need [Docker](https://www.docker.com/) installed and running.

### Download CWL files

Download [the archive with CWL files](lsq-clean-cwl.zip)

```shell
wget lsq-clean-cwl.zip
```

Decompress the file and move inside the created directory (the following commands assume you are inside the directory).

```shell
unzip lsq-clean-cwl.zip
cd lsq-clean-cwl
```

### Install a CWL Engine

For the supplied pipelines you need a CWL engine supporting [CWL v1.2.0](https://www.commonwl.org/v1.2/), for example [cwltool](https://github.com/common-workflow-language/cwltool).

If you have not it installed globally, you can install cwltool locally with Python and PIP following the steps below.

Install virtualenv (if you don't already have it installed)

```shell
pip install virtualenv
```

Create a new virtual environment and install the requirements (including cwltool)

```shell
virtualenv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Run CWL Pipeline

To convert a single file you can use `simple-workflow.cwl`. 
You can find an example of input in `simple-input-example.yml`.

```shell
cwl-runner --outdir=data/output \
    cwl/simple-workflow.cwl cwl/simple-input-example.yml
```

To convert multiple files you can use `list-workflow.cwl`. 
You can find an example of input in `sample-files.yml`, featuring some files from the LSQ 2 dataset.

```shell
cwl-runner --outdir=data/output \
    cwl/list-workflow.cwl cwl/sample-files.yml
```

To convert multiple files under a common namespace (with a simpler input in respect to `list-workflow.cwl`) you can use `remote-sources-workflow.cwl`. 
You can find an example of input in `sample-sources.yml`, featuring some files from the LSQ 2 dataset.

```shell
cwl-runner --outdir=data/output \
    cwl/remote-sources-workflow.cwl cwl/sample-sources.yml
```

## Build

Install nodejs dependencies

```shell
npm install
```

Publish new Docker image

```shell
source publishImage.sh
```